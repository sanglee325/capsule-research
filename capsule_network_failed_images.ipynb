{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2dff8adae10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import resources\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# random seed (for reproducibility)\n",
    "seed = 1\n",
    "# set random seed for numpy\n",
    "np.random.seed(seed)\n",
    "# set random seed for pytorch\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# convert data to Tensors\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(root='data', train=True,\n",
    "                            download=True, transform=transform)\n",
    "\n",
    "test_data = datasets.MNIST(root='data', train=False, \n",
    "                           download=True, transform=transform)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, \n",
    "                                           batch_size=batch_size, \n",
    "                                           num_workers=num_workers)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, \n",
    "                                          batch_size=batch_size, \n",
    "                                          num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capsule Network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available!\n"
     ]
    }
   ],
   "source": [
    "# GPUs\n",
    "# it will also be relevant, in this model, to see if I can train on gpu\n",
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_cuda else 'cpu')\n",
    "if is_cuda:\n",
    "    print(\"CUDA available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=1, out_channels=256):\n",
    "        '''Constructs the ConvLayer with a specified input and output size.\n",
    "           param in_channels: input depth of an image, default value = 1\n",
    "           param out_channels: output depth of the convolutional layer, default value = 256\n",
    "           '''\n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        # defining a convolutional layer of the specified size\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, \n",
    "                              kernel_size=9, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Defines the feedforward behavior.\n",
    "           param x: the input to the layer; an input image\n",
    "           return: a relu-activated, convolutional layer\n",
    "           '''\n",
    "        # applying a ReLu activation to the outputs of the conv layer\n",
    "        features = F.relu(self.conv(x)) # will have dimensions (batch_size, 20, 20, 256)\n",
    "        return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_capsules=8, in_channels=256, out_channels=32):\n",
    "        '''Constructs a list of convolutional layers to be used in \n",
    "           creating capsule output vectors.\n",
    "           param num_capsules: number of capsules to create\n",
    "           param in_channels: input depth of features, default value = 256\n",
    "           param out_channels: output depth of the convolutional layers, default value = 32\n",
    "           '''\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "\n",
    "        # creating a list of convolutional layers for each capsule I want to create\n",
    "        # all capsules have a conv layer with the same parameters\n",
    "        self.capsules = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                      kernel_size=9, stride=2, padding=0)\n",
    "            for _ in range(num_capsules)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''Defines the feedforward behavior.\n",
    "           param x: the input; features from a convolutional layer\n",
    "           return: a set of normalized, capsule output vectors\n",
    "           '''\n",
    "        # get batch size of inputs\n",
    "        batch_size = x.size(0)\n",
    "        # reshape convolutional layer outputs to be (batch_size, vector_dim=1152, 1)\n",
    "        u = [capsule(x).view(batch_size, 32 * 6 * 6, 1) for capsule in self.capsules]\n",
    "        # stack up output vectors, u, one for each capsule\n",
    "        u = torch.cat(u, dim=-1)\n",
    "        # squashing the stack of vectors\n",
    "        u_squash = self.squash(u)\n",
    "        return u_squash\n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        '''Squashes an input Tensor so it has a magnitude between 0-1.\n",
    "           param input_tensor: a stack of capsule inputs, s_j\n",
    "           return: a stack of normalized, capsule output vectors, v_j\n",
    "           '''\n",
    "        squared_norm = (input_tensor ** 2).sum(dim=-1, keepdim=True)\n",
    "        scale = squared_norm / (1 + squared_norm) # normalization coeff\n",
    "        output_tensor = scale * input_tensor / torch.sqrt(squared_norm)    \n",
    "        return output_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers # to get transpose softmax function\n",
    "\n",
    "# dynamic routing\n",
    "def dynamic_routing(b_ij, u_hat, squash, routing_iterations=3):\n",
    "    '''Performs dynamic routing between two capsule layers.\n",
    "       param b_ij: initial log probabilities that capsule i should be coupled to capsule j\n",
    "       param u_hat: input, weighted capsule vectors, W u\n",
    "       param squash: given, normalizing squash function\n",
    "       param routing_iterations: number of times to update coupling coefficients\n",
    "       return: v_j, output capsule vectors\n",
    "       '''    \n",
    "    # update b_ij, c_ij for number of routing iterations\n",
    "    for iteration in range(routing_iterations):\n",
    "        # softmax calculation of coupling coefficients, c_ij\n",
    "        c_ij = helpers.softmax(b_ij, dim=2)\n",
    "\n",
    "        # calculating total capsule inputs, s_j = sum(c_ij*u_hat)\n",
    "        s_j = (c_ij * u_hat).sum(dim=2, keepdim=True)\n",
    "\n",
    "        # squashing to get a normalized vector output, v_j\n",
    "        v_j = squash(s_j)\n",
    "\n",
    "        # if not on the last iteration, calculate agreement and new b_ij\n",
    "        if iteration < routing_iterations - 1:\n",
    "            # agreement\n",
    "            a_ij = (u_hat * v_j).sum(dim=-1, keepdim=True)\n",
    "            \n",
    "            # new b_ij\n",
    "            b_ij = b_ij + a_ij\n",
    "    \n",
    "    return v_j # return latest v_j\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitCaps(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_capsules=10, previous_layer_nodes=32*6*6, \n",
    "                 in_channels=8, out_channels=16):\n",
    "        '''Constructs an initial weight matrix, W, and sets class variables.\n",
    "           param num_capsules: number of capsules to create\n",
    "           param previous_layer_nodes: dimension of input capsule vector, default value = 1152\n",
    "           param in_channels: number of capsules in previous layer, default value = 8\n",
    "           param out_channels: dimensions of output capsule vector, default value = 16\n",
    "           '''\n",
    "        super(DigitCaps, self).__init__()\n",
    "\n",
    "        # setting class variables\n",
    "        self.num_capsules = num_capsules\n",
    "        self.previous_layer_nodes = previous_layer_nodes # vector input (dim=1152)\n",
    "        self.in_channels = in_channels # previous layer's number of capsules\n",
    "\n",
    "        # starting out with a randomly initialized weight matrix, W\n",
    "        # these will be the weights connecting the PrimaryCaps and DigitCaps layers\n",
    "        self.W = nn.Parameter(torch.randn(num_capsules, previous_layer_nodes, \n",
    "                                          in_channels, out_channels))\n",
    "\n",
    "    def forward(self, u):\n",
    "        '''Defines the feedforward behavior.\n",
    "           param u: the input; vectors from the previous PrimaryCaps layer\n",
    "           return: a set of normalized, capsule output vectors\n",
    "           '''\n",
    "        \n",
    "        # adding batch_size dims and stacking all u vectors\n",
    "        u = u[None, :, :, None, :]\n",
    "        # 4D weight matrix\n",
    "        W = self.W[:, None, :, :, :]\n",
    "        \n",
    "        # calculating u_hat = W*u\n",
    "        u_hat = torch.matmul(u, W)\n",
    "\n",
    "        # getting the correct size of b_ij\n",
    "        # setting them all to 0, initially\n",
    "        b_ij = torch.zeros(*u_hat.size())\n",
    "        \n",
    "        # moving b_ij to GPU, if available\n",
    "        if is_cuda:\n",
    "            b_ij = b_ij.to(device)\n",
    "\n",
    "        # update coupling coefficients and calculate v_j\n",
    "        v_j = dynamic_routing(b_ij, u_hat, self.squash, routing_iterations=3)\n",
    "\n",
    "        return v_j # return final vector outputs\n",
    "    \n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        '''Squashes an input Tensor so it has a magnitude between 0-1.\n",
    "           param input_tensor: a stack of capsule inputs, s_j\n",
    "           return: a stack of normalized, capsule output vectors, v_j\n",
    "           '''\n",
    "        # same squash function as before\n",
    "        squared_norm = (input_tensor ** 2).sum(dim=-1, keepdim=True)\n",
    "        scale = squared_norm / (1 + squared_norm) # normalization coeff\n",
    "        output_tensor = scale * input_tensor / torch.sqrt(squared_norm)    \n",
    "        return output_tensor\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_vector_length=16, input_capsules=10, hidden_dim=512):\n",
    "        '''Constructs an series of linear layers + activations.\n",
    "           param input_vector_length: dimension of input capsule vector, default value = 16\n",
    "           param input_capsules: number of capsules in previous layer, default value = 10\n",
    "           param hidden_dim: dimensions of hidden layers, default value = 512\n",
    "           '''\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # calculate input_dim\n",
    "        input_dim = input_vector_length * input_capsules\n",
    "        \n",
    "        # define linear layers + activations\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), # first hidden layer\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, hidden_dim*2), # second, twice as deep\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim*2, 28*28), # can be reshaped into 28*28 image\n",
    "            nn.Sigmoid() # sigmoid activation to get output pixel values in a range from 0-1\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Defines the feedforward behavior.\n",
    "           param x: the input; vectors from the previous DigitCaps layer\n",
    "           return: two things, reconstructed images and the class scores, y\n",
    "           '''\n",
    "        classes = (x ** 2).sum(dim=-1) ** 0.5\n",
    "        classes = F.softmax(classes, dim=-1)\n",
    "        \n",
    "        # find the capsule with the maximum vector length\n",
    "        # here, vector length indicates the probability of a class' existence\n",
    "        _, max_length_indices = classes.max(dim=1)\n",
    "        \n",
    "        # create a sparse class matrix\n",
    "        sparse_matrix = torch.eye(10) # 10 is the number of classes\n",
    "        if is_cuda:\n",
    "            sparse_matrix = sparse_matrix.to(device)\n",
    "        # get the class scores from the \"correct\" capsule\n",
    "        y = sparse_matrix.index_select(dim=0, index=max_length_indices.data)\n",
    "        \n",
    "        # create reconstructed pixels\n",
    "        x = x * y[:, :, None]\n",
    "        # flatten image into a vector shape (batch_size, vector_dim)\n",
    "        flattened_x = x.view(x.size(0), -1)\n",
    "        # create reconstructed image vectors\n",
    "        reconstructions = self.linear_layers(flattened_x)\n",
    "        \n",
    "        # return reconstructions and the class scores, y\n",
    "        return reconstructions, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''Constructs a complete Capsule Network.'''\n",
    "        super(CapsuleNetwork, self).__init__()\n",
    "        self.conv_layer = ConvLayer()\n",
    "        self.primary_capsules = PrimaryCaps()\n",
    "        self.digit_capsules = DigitCaps()\n",
    "        self.decoder = Decoder()\n",
    "                \n",
    "    def forward(self, images):\n",
    "        '''Defines the feedforward behavior.\n",
    "           param images: the original MNIST image input data\n",
    "           return: output of DigitCaps layer, reconstructed images, class scores\n",
    "           '''\n",
    "        primary_caps_output = self.primary_capsules(self.conv_layer(images))\n",
    "        caps_output = self.digit_capsules(primary_caps_output).squeeze().transpose(0,1)\n",
    "        reconstructions, y = self.decoder(caps_output)\n",
    "        return caps_output, reconstructions, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH = './model/capsule_net.pth'\n",
    "capsule_net = CapsuleNetwork().to(device)\n",
    "capsule_net.load_state_dict(torch.load(MODEL_PATH, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''Constructs a CapsuleLoss module.'''\n",
    "        super(CapsuleLoss, self).__init__()\n",
    "        self.reconstruction_loss = nn.MSELoss(reduction='sum') # cumulative loss, equiv to size_average=False\n",
    "\n",
    "    def forward(self, x, labels, images, reconstructions):\n",
    "        '''Defines how the loss compares inputs.\n",
    "           param x: digit capsule outputs\n",
    "           param labels: \n",
    "           param images: the original MNIST image input data\n",
    "           param reconstructions: reconstructed MNIST image data\n",
    "           return: weighted margin and reconstruction loss, averaged over a batch\n",
    "           '''\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        ##  calculate the margin loss   ##\n",
    "        \n",
    "        # get magnitude of digit capsule vectors, v_c\n",
    "        v_c = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n",
    "\n",
    "        # calculate \"correct\" and incorrect loss\n",
    "        left = F.relu(0.9 - v_c).view(batch_size, -1)\n",
    "        right = F.relu(v_c - 0.1).view(batch_size, -1)\n",
    "        \n",
    "        # sum the losses, with a lambda = 0.5\n",
    "        margin_loss = labels * left + 0.5 * (1. - labels) * right\n",
    "        margin_loss = margin_loss.sum()\n",
    "\n",
    "        ##  calculate the reconstruction loss   ##\n",
    "        images = images.view(reconstructions.size()[0], -1)\n",
    "        reconstruction_loss = self.reconstruction_loss(reconstructions, images)\n",
    "\n",
    "        # return a weighted, summed loss, averaged over a batch size\n",
    "        return (margin_loss + 0.0005 * reconstruction_loss) / images.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# custom loss\n",
    "criterion = CapsuleLoss()\n",
    "\n",
    "# Adam optimizer with default params\n",
    "optimizer = optim.Adam(capsule_net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trained Capsule Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(capsule_net, test_loader):\n",
    "    '''Prints out test statistics for a given capsule net.\n",
    "       param capsule_net: trained capsule network\n",
    "       param test_loader: test dataloader\n",
    "       return: returns last batch of test image data and corresponding reconstructions\n",
    "       '''\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    failed_images = []\n",
    "    \n",
    "    test_loss = 0 # loss tracking\n",
    "\n",
    "    capsule_net.eval() # eval mode\n",
    "\n",
    "    for batch_i, (images, target) in enumerate(test_loader):\n",
    "        target = torch.eye(10).index_select(dim=0, index=target)\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        if is_cuda:\n",
    "            images, target = images.to(device), target.to(device)\n",
    "\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        caps_output, reconstructions, y = capsule_net(images)\n",
    "        # calculate the loss\n",
    "        loss = criterion(caps_output, target, images, reconstructions)\n",
    "        # update average test loss \n",
    "        test_loss += loss.item()\n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(y.data.cpu(), 1)\n",
    "        _, target_shape = torch.max(target.data.cpu(), 1)\n",
    "\n",
    "        # compare predictions to true label\n",
    "        correct = np.squeeze(pred.eq(target_shape.data.view_as(pred)))\n",
    "        # calculate test accuracy for each object class\n",
    "        for i in range(batch_size):\n",
    "            label = target_shape.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "            if correct[i] == False:\n",
    "                num = batch_i*batch_size + i\n",
    "                failed_images.append((images[i], target_shape[i].item(), pred[i].item(), num))\n",
    "                \n",
    "\n",
    "    # avg test loss\n",
    "    avg_test_loss = test_loss/len(test_loader)\n",
    "    print('Test Loss: {:.8f}\\n'.format(avg_test_loss))\n",
    "\n",
    "    for i in range(10):\n",
    "        if class_total[i] > 0:\n",
    "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "                str(i), 100 * class_correct[i] / class_total[i],\n",
    "                np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "        else:\n",
    "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "        100. * np.sum(class_correct) / np.sum(class_total),\n",
    "        np.sum(class_correct), np.sum(class_total)))\n",
    "    \n",
    "    # return last batch of capsule vectors, images, reconstructions\n",
    "    return caps_output, images, reconstructions, failed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.02263528\n",
      "\n",
      "Test Accuracy of     0: 99% (979/980)\n",
      "Test Accuracy of     1: 99% (1132/1135)\n",
      "Test Accuracy of     2: 99% (1024/1032)\n",
      "Test Accuracy of     3: 99% (1001/1010)\n",
      "Test Accuracy of     4: 98% (969/982)\n",
      "Test Accuracy of     5: 99% (885/892)\n",
      "Test Accuracy of     6: 98% (946/958)\n",
      "Test Accuracy of     7: 99% (1018/1028)\n",
      "Test Accuracy of     8: 99% (967/974)\n",
      "Test Accuracy of     9: 98% (996/1009)\n",
      "\n",
      "Test Accuracy (Overall): 99% (9917/10000)\n"
     ]
    }
   ],
   "source": [
    "# call test function and get reconstructed images\n",
    "caps_output, images, reconstructions, failed_images = test(capsule_net, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Incorrect Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "SAVE_PATH = './failed_images/'\n",
    "cnt=0\n",
    "for i, (image, target, pred, num) in enumerate(failed_images):\n",
    "    title = SAVE_PATH + str(num) + '_[' + str(target) + '_' + str(pred) +'].png'\n",
    "    save_image(image, title)\n",
    "    '''\n",
    "    cnt+=1\n",
    "    plt.subplot(5, len(failed_images), cnt)\n",
    "    if i == 0:\n",
    "        plt.ylabel(str(i))\n",
    "        plt.title('{} -> {}'.format(target, pred))\n",
    "        title = SAVE_PATH + str(i) + '_[' + str(target) + '_' + str(pred) +'].png'\n",
    "        save_image(image, title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 784])\n"
     ]
    }
   ],
   "source": [
    "print(reconstructions.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python36964bitpytorchconda6cf19ac9f34f4588ad6ad06c9a8ebc05"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
